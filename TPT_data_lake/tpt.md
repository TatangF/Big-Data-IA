============================================= Listes des différents Fichiers ===========================================================================
Client1.csv
Client2.csv
Catalogue.csv
CO2.cvs
Immatriculation.cvs
Marketing.csv

============================================= ACTIVITES DANS ORACLE NOSQL ===========================================================================
nohup java -Xmx64m -Xms64m -jar $KVHOME/lib/kvstore.jar kvlite -secure-config disable -root $KVROOT &

java -jar $KVHOME/lib/kvstore.jar runadmin -port 5000 -host localhost


--Vérifier si la base NoSQL est en cours d'exécution
$ java -jar $KVHOME/lib/kvstore.jar ping -host localhost -port 5000
-- reponse : KVStore ping succeeded in 34 ms.


-- sinon redemarer 
$ java -jar $KVHOME/lib/kvstore.jar start -root $KVROOT -host localhost -port 5000


kv->connect store -name kvstore

## Charger catalogue dans oracleNosql

-- creation de la table catalogue
execute 'drop table catalogue'
execute 'Create table catalogue (
marque string, 
nom string, 
puissance integer, 
longueur string, 
nbPlaces integer, 
nbPortes integer, 
couleur string, 
occasion string, 
prix integer,
id NUMBER GENERATED BY DEFAULT AS IDENTITY, PRIMARY KEY (id))'

-- chargement des données
put table -name catalogue -file /vagrant/TPT/catalogue.json


-- vérifier
execute 'select * from catalogue'

============================================= ACTIVITES DANS HADOOP HDFS ===========================================================================
# Charger Immatriculation dans HADOOP
hadoop fs -put /vagrant/TPT/Immatriculations.csv

# Charger CO2 dans HADOOP
hadoop fs -put /vagrant/TPT/CO2.csv

#Charger Clients_31 dans HADOOP
hadoop fs -put /vagrant/TPT/Clients_31.csv


============================================= ACTIVITES DANS MongoDB===========================================================================
## Charger Marketing dans MongoDB
>  use tpt
switched to db tpt
> db.creatCollection("Marketing");
-- show collections;

-- Approche  : via ``mongoimport`` cvs
-- Les données sont dans les fichiers :
-- Marketing.csv

db.tpt.remove({});

mongoimport --db tpt --collection Marketing --file /vagrant/TPT/Marketing.csv --type csv --ignoreBlanks --headerline



============================================= ACTIVITES DANS Hive ===========================================================================
-- préliminaire 
----Hive
---Start Hive (Metastore service & HiveServer2)
nohup hive --service metastore > /dev/null &
nohup hiveserver2 > /dev/null &



beeline

beeline>   !connect jdbc:hive2://localhost:10000 
----or
beeline -u jdbc:hive2://localhost:10000 vagrant

Enter username for jdbc:hive2://localhost:10000: oracle
Enter password for jdbc:hive2://localhost:10000: ********
(password : welcome1)


## Charger un des fichiers clients2 dans Hive
-- on crée une table clients2 avec les caratéristiques de chaque colonne puis 
-- on charges les données dans la table crée pars notre fichier csv

drop table Clients_32;
CREATE TABLE Clients_32(
age  FLOAT,
sexe string,
taux  FLOAT,
situationFamiliale string,
nbEnfantsAcharge string,
2eme_voiture BOOLEAN,
immatriculation string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


--- charger les données de votre fichier CSV dans la table en utilisant la commande suivante
LOAD DATA LOCAL INPATH '/vagrant/TPT/Clients_32.csv' INTO TABLE Clients_32;

-- Vérification
select * from Clients_32 limit 5;



--Note :  la clause STORED AS TEXTFILE indique à Hive que les données seront stockées sous forme de fichier texte brut sur le système de fichiers HDFS. Cela signifie 
--que les données seront stockées sous forme de fichiers simples, sans formatage ni compression,
--- ce qui peut faciliter leur traitement par Hive.

##Construction de toutes les tables EXTERNES ( Client_31_hive_ext, Catalogue_hive_ext, 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++ création de tables externes HIVE pour une table hadoop+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

##  CO2_hive_ext,Immatriculation_hive_ext, Marketing_hive_ext)

-- Table externe `Clients_31`
--aller dans hadoop : 	hadoop fs -mkdir /clients
						hadoop fs -mv Clients_31.csv/clients

drop table Client_31_hive_ext;
CREATE EXTERNAL TABLE Client_31_hive_ext(
age  FLOAT,
sexe string,
taux  FLOAT,
situationFamiliale string,
nbEnfantsAcharge string,
2eme_voiture BOOLEAN,
immatriculation string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:/clients'
TBLPROPERTIES ("skip.header.line.count"="1");

-- vérifier
select * from Client_31_hive_ext limit 5;

-- TABLE externe `CO2`
--aller dans hadoop : 	hadoop fs -mkdir /carbone
						hadoop fs -mv CO2.csv /carbone
drop table CO2_hive_ext;
CREATE EXTERNAL TABLE CO2_hive_ext(
MarqueModele string,
BonusMalus string,
RejetsCO2gkm string,
Coutenerie string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE 
LOCATION 'hdfs:/carbone'
TBLPROPERTIES ("skip.header.line.count"="1");

--vérifier
select * from CO2_hive_ext limit 5;

-- TABLE EXTERNES 'Immatriculations'
--aller dans hadoop : 	hadoop fs -mkdir /Immatriculations
						hadoop fs -mv Immatriculations.csv /Immatriculations

drop table Immatriculations_hive_ext;
CREATE EXTERNAL TABLE Immatriculations_hive_ext(
Immatriculation string,
Marque string,
Nom string,
Puissance FLOAT,
Longueur string,
NbPlaces FLOAT, 
NbPortes FLOAT,
Couleur string,
Occasion BOOLEAN,
Prix FLOAT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE 
LOCATION 'hdfs:/Immatriculations'
TBLPROPERTIES ("skip.header.line.count"="1");

select * from Immatriculations_hive_ext limit 5;


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++création de table externe HIVE pour une table oracle NoSQL++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

drop table catalogue_hive_ext;
CREATE EXTERNAL TABLE catalogue_hive_ext (
marque string, 
nom string, 
puissance int, 
longueur string, 
nbPlaces integer, 
nbPortes int, 
couleur string, 
occasion string, 
prix int,
id int)
STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler'
TBLPROPERTIES ( 
"oracle.kv.kvstore" = "kvstore", 
"oracle.kv.hosts" =  "localhost:5000", 
"oracle.kv.hadoop.hosts" = "localhost/127.0.0.1", 
"oracle.kv.tableName" = "catalogue");

-- vérifie
select * from catalogue_hive_ext limit 5;

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++création de table externe HIVE pour une table MongoDB++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

drop table Marketing_hive_mongo_ext;
CREATE EXTERNAL TABLE Marketing_hive_mongo_ext (
id INT,
age INT,
sexe string,
taux FLOAT,
situationFamiliale string,
nbEnfantsAcharge INT,
2eme_voiture string
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
WITH SERDEPROPERTIES('mongo.columns.mapping'='{
"id":"_id",
"situationFamiliale":"situationFamiliale", 
"nbEnfantsAcharge": "nbEnfantsAcharge", 
"2eme_voiture" : "2eme_voiture"}')
TBLPROPERTIES ('mongo.uri'='mongodb://localhost:27017/tpt.Marketing');

SELECT * FROM Marketing_hive_mongo_ext LIMIT 5;

describe Marketing_hive_mongo_ext;

select age, situationfamiliale, nbenfantsacharge, 2eme_voiture from Marketing_hive_mongo_ext limit 5;

length(Marketing_hive_mongo_ext)






-- vérifier toutes les tables
show tables;
+----------------------------+
|          tab_name          |
+----------------------------+
| catalogue_hive_ext         |
| client_31_hive_ext         |
| clients_32                 |
| co2_hive_ext               |
| emp_hive_hadoop_ext        |
| immatriculations_hive_ext  |
| marketing_hive_mongo_ext   |
| userprofile_hive_ext       |
+----------------------------+

============================================= 	Outils D'analyse des Données: SPARK VIA PYTHON 	 ===========================================================================
-- installer les dépendances pour pip
curl https://bootstrap.pypa.io/pip/3.6/get-pip.py -o get-pip.py
python3 get-pip.py --user

--vous pouvez utiliser la commande pour installer Jupyter Notebook
~/.local/bin/pip install jupyter 

--lancer l'interface utilisateur de Jupyter Notebook en exécutant la commande 
~/.local/bin/jupyter notebook 
or 
jupyter notebook --ip=0.0.0.0 --port=9000 --no-browser



=============================================	Map/ Reduce	 ===========================================================================

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("MyApp")

sc = SparkContext(conf=conf)


--- rdd qui permet de sépaer les Marques et les Modèles
# créer un RDD à partir de la DataFrame
rdd_CO = df_CO.rdd

# Appliquer la méthode `map()` pour séparer la première colonne en deux
rdd_separated = rdd_CO.map(lambda x: (x[0], x[1].split(" ")[0], " ".join(x[1].split(" ")[1:]), *x[2:]))

# Convertir le RDD en DataFrame en spécifiant les noms de colonne
df_separated = rdd_separated.toDF(["Marque", "Modele", "Colonne3", "Colonne4", ...])

# afficher le contenu du RDD séparé
rdd_separated.collect()


=============================================	Configuration de python et Oracle Nosql	 ===========================================================================

Créez un répertoire pour extraire les fichiers. 
Par exemple, vous pouvez créer un répertoire instantclient_21_9 dans votre répertoire personnel :

mkdir ~/instantclient

--déplacer le fichier zip dans 
mv /vagrant/instantclient-basic-windows.x64-21.9.0.0.0dbru.zip ~/instantclient

-- acceder au répertoire et déziper le fichier zip
cd ~/instantclient
unzip instantclient-basic-windows.x64-21.9.0.0.0dbru.zip

--Définir la variable d'environnement LD_LIBRARY_PATH pour inclure le chemin vers le ré^pertoire créer
export LD_LIBRARY_PATH=$HOME/instantclient:$LD_LIBRARY_PATH

export LD_LIBRARY_PATH=/usr/lib/oracle/instantclient_21_3:$LD_LIBRARY_PATH



------Nouvelle approche--------------


Installer le package oci-nosql en utilisant pip:

pip install oci-nosql

import oci.nosql

--Configurer les paramètres de connexion pour votre instance Oracle NoSQL en créant un objet oci.nosql.NoSQLHandle:

config = {
    "endpoint": "localhost:5000",
    "security": {
        "mode": "none"
    }
}

handle = oci.nosql.NoSQLHandle(config)

--Utiliser la méthode get_table pour accéder à une table spécifique:

table = handle.get_table("catalogue")

--Utiliser les méthodes de la classe Table pour interagir avec les données de la table:

# Exemple : récupérer toutes les lignes de la table
rows = table.table_operations.get_table(
    limit=-1,  # récupérer toutes les lignes
    consistency=oci.nosql.models.Consistency.ABSOLUTE
).rows

for row in rows:
    print(row)
	
	
============================================	Configuration de python et HIVE	 ===========================================================================
-- Installation des packages
PyHive 
thrift
sasl
findspark
pyodbc


