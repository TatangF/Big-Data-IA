============================================= Listes des différents Fichiers ===========================================================================
Client1.csv
Client2.csv
Catalogue.csv
CO2.cvs
Immatriculation.cvs
Marketing.csv


============================================= ACTIVITES DANS ORACLE NOSQL ===========================================================================
__demarer oracle  
$ java -jar $KVHOME/lib/kvstore.jar start -root $KVROOT -host localhost -port 5000
__vous allez avoire ce message 
kv->
__verifier que vous etes connecter, avec cette commande
connect store -name kvstore

__Charger les données avec ces lignes
 
 ## Charger le fichier catalogue.cvs dans oracleNosql
 
__creation de la table catalogue
execute 'drop table catalogue'
execute 'Create table catalogue (
marque string, 
nom string, 
puissance integer, 
longueur string, 
nbPlaces integer, 
nbPortes integer, 
couleur string, 
occasion string, 
prix integer,
id NUMBER GENERATED BY DEFAULT AS IDENTITY, PRIMARY KEY (id))'

__chargement des données avec cette ligne 
put table -name catalogue -file /vagrant/TPT/catalogue.json

__vérifier que vos données sont présentes
execute 'select * from catalogue limit 5'

============================================= ACTIVITES DANS HADOOP HDFS ===========================================================================
# Charger Immatriculation dans HADOOP
hadoop fs -put /vagrant/TPT/Immatriculations.csv

# Charger CO2 dans HADOOP
hadoop fs -put /vagrant/TPT/CO2.csv

#Charger Clients_31 dans HADOOP
hadoop fs -put /vagrant/TPT/Clients_31.csv


============================================= ACTIVITES DANS MongoDB===========================================================================
##Entrer
mongo

## Charger Marketing dans MongoDB
__REGARDEZ LES DIFFERENTES BASE DE DONNEES 
show dbs
__utiliser la base de données tpt avec cette commande et si elle n'exite pas elle seras crée
use tpt
__crée la collection Marketing avec commande 
db.creatCollection("Marketing");
__visualiser la collection avec la commande
show collections;

__Insertion des documments, nous allons utiliser l'approche mongoimport

mongoimport --db tpt --collection Marketing --file /vagrant/TPT/Marketing.csv --type csv --ignoreBlanks --headerline
__si vous rencontrez des erreurs execute cette ligne et rellance mongoimport
db.tpt.remove({});
__elle permet en gros de supprimer tous les documents dans la base tpt

__Verification
db.Marketing.find()

============================================= ACTIVITES DANS Hive ===========================================================================
__Préliminaire EXECUTE CETTE COMMANDE POUR DEMARER ----Hive----
----Start Hive (Metastore service & HiveServer2) 
nohup hive --service metastore > /dev/null &
nohup hiveserver2 > /dev/null &

__EXECUTE cette commande 
beeline
__et ensuite celle-ci
!connect jdbc:hive2://localhost:10000

Enter username for jdbc:hive2://localhost:10000: oracle
Enter password for jdbc:hive2://localhost:10000: ********
(password : welcome1)

__elle permettent de ce connecter a hive et si tu obtient une erreure execute la ligne de commande suivante
beeline -u jdbc:hive2://localhost:10000 vagrant
__elle va te connecter directement a hive

## Charger un des fichiers clients2 dans Hive
-- on crée une table clients2 avec les caratéristiques de chaque colonne puis 
-- on charges les données dans la table crée pars notre fichier csv

drop table Clients_32;
CREATE TABLE Clients_32(
age  FLOAT,
sexe string,
taux  FLOAT,
situationFamiliale string,
nbEnfantsAcharge string,
2eme_voiture BOOLEAN,
immatriculation string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;


--- charger les données de votre fichier CSV dans la table en utilisant la commande suivante
LOAD DATA LOCAL INPATH '/vagrant/TPT/Clients_32.csv' INTO TABLE Clients_32;

-- Vérification
select * from Clients_32 limit 5;



--Note :  la clause STORED AS TEXTFILE indique à Hive que les données seront stockées sous forme de fichier texte brut sur le système de fichiers HDFS. Cela signifie 
--que les données seront stockées sous forme de fichiers simples, sans formatage ni compression,
--- ce qui peut faciliter leur traitement par Hive.

##Construction de toutes les tables EXTERNES ( Client_31_hive_ext, Catalogue_hive_ext, 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++ création de tables externes HIVE pour une table hadoop+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

##  CO2_hive_ext,Immatriculation_hive_ext, Marketing_hive_ext)

-- Table externe `Clients_31`
--aller dans hadoop : 	hadoop fs -mkdir /clients
						hadoop fs -mv Clients_31.csv/clients

drop table Client_31_hive_ext;
CREATE EXTERNAL TABLE Client_31_hive_ext(
age  FLOAT,
sexe string,
taux  FLOAT,
situationFamiliale string,
nbEnfantsAcharge string,
2eme_voiture BOOLEAN,
immatriculation string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:/clients'
TBLPROPERTIES ("skip.header.line.count"="1");

-- vérifier
select * from Client_31_hive_ext limit 5;

-- TABLE externe `CO2`
--aller dans hadoop : 	hadoop fs -mkdir /carbone
						hadoop fs -mv CO2.csv /carbone
drop table CO2_hive_ext;
CREATE EXTERNAL TABLE CO2_hive_ext(
MarqueModele string,
BonusMalus string,
RejetsCO2gkm string,
Coutenerie string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE 
LOCATION 'hdfs:/carbone'
TBLPROPERTIES ("skip.header.line.count"="1");

--vérifier
select * from CO2_hive_ext limit 5;

-- TABLE EXTERNES 'Immatriculations'
--aller dans hadoop : 	hadoop fs -mkdir /Immatriculations
						hadoop fs -mv Immatriculations.csv /Immatriculations

drop table Immatriculations_hive_ext;
CREATE EXTERNAL TABLE Immatriculations_hive_ext(
Immatriculation string,
Marque string,
Nom string,
Puissance FLOAT,
Longueur string,
NbPlaces FLOAT, 
NbPortes FLOAT,
Couleur string,
Occasion BOOLEAN,
Prix FLOAT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS TEXTFILE 
LOCATION 'hdfs:/Immatriculations'
TBLPROPERTIES ("skip.header.line.count"="1");

select * from Immatriculations_hive_ext limit 5;


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++création de table externe HIVE pour une table oracle NoSQL++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

drop table catalogue_hive_ext;
CREATE EXTERNAL TABLE catalogue_hive_ext (
marque string, 
nom string, 
puissance int, 
longueur string, 
nbPlaces integer, 
nbPortes int, 
couleur string, 
occasion string, 
prix int,
id int)
STORED BY 'oracle.kv.hadoop.hive.table.TableStorageHandler'
TBLPROPERTIES ( 
"oracle.kv.kvstore" = "kvstore", 
"oracle.kv.hosts" =  "localhost:5000", 
"oracle.kv.hadoop.hosts" = "localhost/127.0.0.1", 
"oracle.kv.tableName" = "catalogue");

-- vérifie
select * from catalogue_hive_ext limit 5;

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++création de table externe HIVE pour une table MongoDB++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

drop table Marketing_hive_mongo_ext;
CREATE EXTERNAL TABLE Marketing_hive_mongo_ext (
id INT,
age INT,
sexe string,
taux FLOAT,
situationFamiliale string,
nbEnfantsAcharge INT,
2eme_voiture string
)
STORED BY 'com.mongodb.hadoop.hive.MongoStorageHandler'
WITH SERDEPROPERTIES('mongo.columns.mapping'='{
"id":"_id",
"situationFamiliale":"situationFamiliale", 
"nbEnfantsAcharge": "nbEnfantsAcharge", 
"2eme_voiture" : "2eme_voiture"}')
TBLPROPERTIES ('mongo.uri'='mongodb://localhost:27017/tpt.Marketing');

__verifier
SELECT * FROM Marketing_hive_mongo_ext LIMIT 5;

describe Marketing_hive_mongo_ext;
+---------------------+------------+--------------------+
|      col_name       | data_type  |      comment       |
+---------------------+------------+--------------------+
| id                  | int        | from deserializer  |
| age                 | int        | from deserializer  |
| sexe                | string     | from deserializer  |
| taux                | float      | from deserializer  |
| situationfamiliale  | string     | from deserializer  |
| nbenfantsacharge    | int        | from deserializer  |
| 2eme_voiture        | string     | from deserializer  |
+---------------------+------------+--------------------+


select age, situationfamiliale, nbenfantsacharge, 2eme_voiture from Marketing_hive_mongo_ext limit 5;
+------+---------------------+-------------------+---------------+
| age  | situationfamiliale  | nbenfantsacharge  | 2eme_voiture  |
+------+---------------------+-------------------+---------------+
| 35   | C�libataire         | 0                 | false         |
| 48   | C�libataire         | 0                 | false         |
| 26   | En Couple           | 3                 | true          |
| 80   | En Couple           | 3                 | false         |
| 21   | C�libataire         | 0                 | false         |
+------+---------------------+-------------------+---------------+


-- vérifier toutes les tables
show tables;
+----------------------------+
|          tab_name          |
+----------------------------+
| catalogue_hive_ext         |
| client_31_hive_ext         |
| clients_32                 |
| co2_hive_ext               |
| emp_hive_hadoop_ext        |
| immatriculations_hive_ext  |
| marketing_hive_mongo_ext   |
| userprofile_hive_ext       |
+----------------------------+


============================================= 	Outils D'analyse des Données: SPARK VIA PYTHON 	 ===========================================================================
-- installer les dépendances pour pip
curl https://bootstrap.pypa.io/pip/3.6/get-pip.py -o get-pip.py
python3 get-pip.py --user

--vous pouvez utiliser la commande pour installer Jupyter Notebook
~/.local/bin/pip install jupyter 

--lancer l'interface utilisateur de Jupyter Notebook en exécutant la commande 
~/.local/bin/jupyter notebook 
or 
jupyter notebook --ip=0.0.0.0 --port=9000 --no-browser



=============================================	Map/ Reduce	 ===========================================================================

from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("MyApp")

sc = SparkContext(conf=conf)


--- rdd qui permet de sépaer les Marques et les Modèles
# créer un RDD à partir de la DataFrame
rdd_CO = df_CO.rdd

# Appliquer la méthode `map()` pour séparer la première colonne en deux
rdd_separated = rdd_CO.map(lambda x: (x[0], x[1].split(" ")[0], " ".join(x[1].split(" ")[1:]), *x[2:]))

# Convertir le RDD en DataFrame en spécifiant les noms de colonne
df_separated = rdd_separated.toDF(["Marque", "Modele", "Colonne3", "Colonne4", ...])

# afficher le contenu du RDD séparé
rdd_separated.collect()




=============================================	__MERCI!	 ===========================================================================





















