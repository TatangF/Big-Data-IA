### **CrÃ©ation dâ€™un Data Lake et Pipeline de Transformation de DonnÃ©es**  

#### **ğŸ”¹ Objectif du projet**  
DÃ©velopper une infrastructure Big Data permettant de centraliser, nettoyer et transformer des volumes importants de donnÃ©es afin de les exploiter via lâ€™intelligence artificielle et le machine learning.  

#### **ğŸ”¹ Description du projet**  

âœ… **Phase 1 : CrÃ©ation du Data Lake**  
- Mise en place dâ€™un **lac de donnÃ©es sous Vagrant**, permettant de stocker de gros volumes de donnÃ©es brutes.  
- Structuration des donnÃ©es pour une meilleure gestion et exploitation.  
- Utilisation de **fichiers CSV, JSON et bases de donnÃ©es** pour alimenter le Data Lake.  

âœ… **Phase 2 : Pipeline de Nettoyage et Transformation des DonnÃ©es**  
- DÃ©veloppement de **pipelines de transformation** pour nettoyer et normaliser les donnÃ©es.  
- Automatisation du prÃ©traitement des donnÃ©es avec **Python et Pandas**.  
- Utilisation de **scikit-learn** pour lâ€™ingÃ©nierie des features et la prÃ©paration des datasets.  

âœ… **Phase 3 : Exploitation des DonnÃ©es avec lâ€™IA**  
- DÃ©ploiement de **modÃ¨les de machine learning** (SVM, Random Forest) pour lâ€™analyse prÃ©dictive.  
- Gestion des versions des modÃ¨les et suivi des expÃ©riences avec **MLflow**.  
- IntÃ©gration des modÃ¨les dans des notebooks interactifs **Jupyter** pour visualisation et Ã©valuation.  

#### **ğŸ”¹ Technologies et outils utilisÃ©s**  
- **Big Data** : Vagrant, Data Lake  
- **Langages** : Python (Pandas, NumPy, Scikit-learn)  
- **Stockage** : Fichiers CSV, JSON, bases de donnÃ©es  
- **Machine Learning** : SVM, Random Forest  
- **Suivi des modÃ¨les** : MLflow  
- **DÃ©veloppement et tests** : Jupyter Notebook  

ğŸ’¡ *Ce projet illustre mon expertise en ingÃ©nierie des donnÃ©es, machine learning et automatisation des pipelines de traitement, dans un environnement Big Data.*  
